config:
  scrapeFilesFolder: scrapes # the sub-directory under the script where scraped files will be stored
  defaultCheckOnMinute: "*" # Set to "*" (with quotes) to by default scrape every time the script is called, if no 'minutes' or 'hours' parameters are supplied for a page below. If an integer is set (such as 1, without quotes), then run a scrape at this time past each hour
  runAtLeastEveryXHours: 3 # if script has not been run for at least X hours, then do an immediate scrape of all pages on next run
  emailAccount: # INSERT EMAIL OR GMAIL ADDRESS THAT WILL BE SENDING ALERT EMAILS HERE
  emailPassword: # INSERT EMAIL PASSWORD HERE -- do not use your normal password, but create an app-specific password for gmail and use it
  emailRecipient: # INSERT EMAIL ADDRESS FOR RECIPIENT OF ALERTS HERE
  mercuryparser: # Insert path to a local mercury-parser executable script, to scrape more complex pages better. More info here: https://github.com/postlight/mercury-parser

pages:
  WikiThisDay:
    url: https://en.wikipedia.org/wiki/Main_Page
    searchWithinTag: div#mp-otd #  Only search for id mp-otd in div. In this case, it contains Wikipedia's "On This Day" box
    hours: 6  # Scrape page only if not been scraped in 6 hours (requires cronjob)
    parser: html # [html|xml] #  If set, return HTML of a scraped page (for RSS feeds, e.g., use 'xml'). Otherwise, returns text, stripping all tags
    onlyAdditions: True  # Optional: Do not track any deletions but only new content that appears on this page
#   json: thejsonattribute # If the scraped file is a json file. The second 'thejsonattribute' parameter, for instance, would grab only the {thejsonattribute: 'Juicy Content that you want here'} content from the json.
#   followhyperlinks:  #  This follows and scrapes all hyperlinks in the original scraped page (or at least, all links within the 'searchWithinTag' html)
#     mercury-parser: true  # This uses mercury parser to scrape each hyperlinked page for tricky pages. See readme for more information
#     articleWithinTags: div.article #  Scrape everything in this css selector in the final page or article scraped from hyperlink
#     articlePageTitleTag: h1 #  The tag that contains the title to the page or article that has been followed

  Example: # this is the pagename that you can manually call with --page [pagename] command line argument
    url: http://www.example.org
    searchWithinTag: div h1 #  Search for h1 tags within div
    minutes: 30 #  Scrape page only if not been scraped in 55 minutes (requires cronjob)
    onlyAdditions: True #  If set, this only alerts for additions rather than also deletions in page
    parser: html # [html|xml] #  If set, return HTML of a scraped page (for RSS feeds, e.g., use 'xml'). Otherwise, returns text, stripping all tags