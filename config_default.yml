config:
	scrapeFilesFolder: scrapes # the sub-directory under the script where scraped files will be stored
	defaultCheckOnMinute: "*" # Set to "*" (with quotes) to by default scrape every time the script is called, if no 'minutes' or 'hours' parameters are supplied for a page below. If an integer is set (such as 1, without quotes), then run a scrape at this time past each hour
	runAtLeastEveryXHours: 3 # if script has not been run for at least X hours, then do an immediate scrape of all pages on next run
	emailAccount: #INSERT EMAIL OR GMAIL ADDRESS THAT WILL BE SENDING ALERT EMAILS HERE
	emailPassword: #INSERT EMAIL PASSWORD HERE -- do not use your normal password, but create an app-specific password for gmail and use it
	emailRecipient: #INSERT EMAIL ADDRESS FOR RECIPIENT OF ALERTS HERE

pages:
	Example: # this is the pagename that you can manually call with --page [pagename] command line argument
		url: http://www.example.org
		searchWithinTag: # optionally, only search for diffs within the following XML tags
			tag: div # only search divs, for example
			attr: # optionally specify a class or id attribute for the 'tag' here
		#hours: 6 # requires cronjob - enable this to only scrape every even 6th or nth hour the script is run (i.e., at 6am, 12pm and 6pm)
		#minutes: 5 # requires cronjob - enable this to scrape every even 5th or nth minute the script is run (i.e., 605pm, 610pm, 615pm, etc)
		#strip: # add this if you want to strip the following tags from the page
		#  - strong # this will strip <strong> tags from the fetched file - useful for malformed XML that could break BeautifulSoup parser
		#onlyAdditions: True # If set, this only alerts for additions rather than also deletions in page
		#json: html # Set this if the scraped file is a json file. The second 'html' paramater, for instance, would grab only the {html: 'abcd'} content from the json.